{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbc25be8-d6f9-4037-8605-3d9d3f85b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import nltk\n",
    "import io\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30315014-7437-489a-8751-002e5539989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4956a31-f211-4ee2-9dd1-53269b1def36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90da2f31-e234-499b-b10c-3aab41e0438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text \n",
    "with open('kindle.txt', encoding='ISO-8859-2') as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa098672-6014-41c9-a415-b49655f8430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence tokenization \n",
    "sent_tokenizer = PunktSentenceTokenizer() \n",
    "sents = sent_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e70b8429-a9e7-4025-b816-90c52e7697a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'Love', 'my', 'Kindle']\n",
      "['I Love my Kindle']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "385d4707-025b-4168-a7c5-793b0d7b87c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming \n",
    "porter_stemmer = PorterStemmer()\n",
    "nltk_tokens = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c680560-ca66-4764-8abf-0d232179624a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: I  Stem: i\n",
      "Actual: Love  Stem: love\n",
      "Actual: my  Stem: my\n",
      "Actual: Kindle  Stem: kindl\n"
     ]
    }
   ],
   "source": [
    "for w in nltk_tokens:\n",
    "    print(f\"Actual: {w}  Stem: {porter_stemmer.stem(w)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "add63486-1482-4f31-8cfa-6e3d048cbc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: I  Lemma: I\n",
      "Actual: Love  Lemma: Love\n",
      "Actual: my  Lemma: my\n",
      "Actual: Kindle  Lemma: Kindle\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "for w in nltk_tokens:\n",
    "    print(f\"Actual: {w}  Lemma: {wordnet_lemmatizer.lemmatize(w)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26d22f57-aeb2-478a-9807-c68576beb518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('Love', 'VBP'), ('my', 'PRP$'), ('Kindle', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "#  POS tagging \n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc38ed94-1e01-4167-9f2a-658f1ce7c0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I Love my Kindle\n",
      "compound: 0.6369, neg: 0.0, neu: 0.323, pos: 0.677, \n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis \n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "with open('kindle.txt', encoding='ISO-8859-2') as f:\n",
    "    for line in f.read().split('\\n'):   # use 'line', not 'text'\n",
    "        print(line)\n",
    "        scores = sid.polarity_scores(line)\n",
    "        for key in sorted(scores):\n",
    "            print(f'{key}: {scores[key]}, ', end='')\n",
    "        print()  # newline for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c47922b-8366-44ee-8fc6-20088a2e0ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
